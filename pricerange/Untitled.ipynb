{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embedding_matrix.pkl\", \"rb\") as f:\n",
    "    embedding_matrix = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "replace_puncts = {'`': \"'\", '′': \"'\", '“':'\"', '”': '\"', '‘': \"'\"}\n",
    "\n",
    "strip_chars = [',', '.', '\"', ':', ')', '(', '-', '|', ';', \"'\", '[', ']', '>', '=', '+', '\\\\', '•',  '~', '@', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√']\n",
    "\n",
    "puncts = ['!', '?', '$', '&', '/', '%', '#', '*','£']\n",
    "\n",
    "def clean_str(x):\n",
    "    x = str(x)\n",
    "    \n",
    "    x = x.lower()\n",
    "    \n",
    "    x = re.sub(r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})\", \"url\", x)\n",
    "    \n",
    "    for k, v in replace_puncts.items():\n",
    "        x = x.replace(k, f' {v} ')\n",
    "        \n",
    "    for punct in strip_chars:\n",
    "        x = x.replace(punct, ' ') \n",
    "    \n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "        \n",
    "    x = x.replace(\" '\", \" \")\n",
    "    x = x.replace(\"' \", \" \")\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.initializers import Constant\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    embeddings_initializer=Constant(embedding_matrix),\n",
    "                    input_length=sequence_length))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units=4, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_reviews(reviews):\n",
    "    for i,j in enumerate(reviews):\n",
    "        reviews[i] = clean_str(j)\n",
    "    sequence_length = 300\n",
    "    max_features = 20000\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_features, split=' ', oov_token='<unw>', filters=' ')\n",
    "    tokenizer.fit_on_texts(reviews)\n",
    "    X = tokenizer.texts_to_sequences(reviews)\n",
    "    X = tf.keras.preprocessing.sequence.pad_sequences(X, sequence_length)\n",
    "    return X\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = process_reviews([\"\"\"After watching a ton of his shows on TV, I've been wanting to try some of his signature items to see if he lives up to his reputation.\n",
    "\n",
    "This is one of his Vegas flagships. It was my first time here but hubby had been here before and loved it. Trendy decor and cute story about going through the Chunnel from Paris to London when you're being seated.\n",
    "\n",
    "Excellent service at the table! The hostesses aren't the most consistent, but the table service is excellent. Nice explanation of the menu and steak cuts.\n",
    "\n",
    "These are the highlights and what I would definitely recommend getting:\n",
    "- Beef Wellington - it's the signature item and sooo good. The layers are perfect and the meat inside is incredibly flavorful\n",
    "- Sticky toffee pudding - another classic dish that totally lived up to the hype. Definitely good for sharing after a big meal.\n",
    "\n",
    "Bread service is lovely with a nice variety to choose from, and we also enjoyed the salad to start (not unique, but a nice starter if you need vegetables).  The Mac and cheese side is totall…\"\"\"\n",
    "                     , \"Great restaurant with great food, so great\"\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 72 unique tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,  14,  28,   6,  29,\n",
       "          7,   8,  30,  31,  32,  15,  33,  16,  34,   4,  35,  36,   7,\n",
       "          8,  17,  37,   4,  38,  18,  39,  40,  19,   4,   8,  41,  42,\n",
       "          5,  43,   7,   8,  44,  45,   9,  46,  47,  48,  49,  20,  10,\n",
       "         50,  51,  16,  20,  52,   3,  53,   9,  54,  55,   3,  56,  57,\n",
       "         58,  59,  60,   2,  61,  21,  62,   4,  63,  64,  22,  65,  66,\n",
       "         67,  68,  11,  69,   2,  23,  70,   2,  71,  72,  73,   2,  74,\n",
       "         75,  10,   2,  23,  11,   5,  76,  12,  77,   7,   2,  78,   3,\n",
       "         79,  80,  81,  24,   2,  82,   3,  83,  15,  84,  25,  85,  86,\n",
       "         87,  88,  89,   9,  90,   2,  17,  91,   3,  92,  26,   2,  93,\n",
       "         24,  94,   3,   2,  95,  96,   5,  97,  98,  99, 100, 101, 102,\n",
       "        103, 104, 105, 106, 107,  19,   4,   2, 108,  25,  26, 109, 110,\n",
       "         14,   6, 111, 112, 113,  11,   5, 114,  27,   6,  12, 115,   4,\n",
       "        116,  21,   3, 117, 118, 119,   2, 120,   4, 121, 122, 123,  10,\n",
       "          6,  12, 124,  18,  22, 125, 126,   2, 127,   3, 128, 129,   5,\n",
       "        130],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,  13, 131,  27,  13, 132, 133,\n",
       "         13]], dtype=int32)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "2/2 [==============================] - 0s 19ms/sample\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.6908046e-01, 7.2722453e-01, 3.0859001e-03, 6.0913945e-04],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(x=X, verbose=1)\n",
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import argmax\n",
    "classes = np.argmax(y, axis=1)\n",
    "classes[0]+1  # adding to zero index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
